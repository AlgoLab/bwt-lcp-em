Experimental Evaluation 
=======================

To evaluate the method proposed in this article, we implemented a
prototype in C, named `bwt-lcp-em` (we will refer to it as `ble` in the
following) that is freely available at
<https://github.com/AlgoLab/bwt-lcp-em>. We compared our method with
other tools specifically designed to index datasets composed by a huge
number of short sequences such as Next-Generation Sequencing read sets.

We have compared `ble` with the original implementation of
[extLCP](https://github.com/BEETL/BEETL) (BEETL), as well as a more
recent version ([BEETL2](https://github.com/giovannarosone/BCR_LCP_GSA)) that implements a fully external
memory approach, an in-memory method
[gsa-is](https://github.com/felipelouza/gsa-is), and two recent external memory
tools: [egsa](https://github.com/felipelouza/egsa) and
[egap](https://github.com/felipelouza/egap). 
Unless explicitly stated otherwise, we have
run all tools with the default values for their runtime parameters.

Our comparison shows that `ble` requires no more than three times the
running time of BEETL, while `BEETL` uses at least
40 times the RAM used by `ble`. Moreover, on datasets made of random
strings (i.e., when l<k), our implementation outperforms
`BEETL` also on the runtime. Since `gsa-is` is an in-memory
approach, its running time is much better than BEETL's or
`ble`'s, but its memory requirements made impossible to run `gsa-is` on
even moderately large instances. More recent tools (`egsa` and `egap`)
are faster than `BEETL` and, when given enough main memory,
than BEETL2. But an experimental analysis with 1GB of RAM
shows that our approach is faster than `egap`, at least for sufficiently
large datasets. Moreover `egsa` has almost always been unable to
complete the computation with 1GB of RAM. With this setting, only
`BEETL2` has been able to successfully process all instances.

We did not perform any comparison with tools that compute the BWT and
the LCP of a single string since we specifically aim to build those data
structures for several sequences at the same time. While concatenating
all sequences in the input set (interposing different sentinels between
all the original sequences) allows to solve our problem with approaches
designed for a single string, the alphabet size would go from
O(Σ) to O(m + Σ). Since our datasets
consists of millions of strings, it is unclear which current
implementations are able to cope with such a huge alphabet.

We compared the three tools in two scenarios, depending on the amount of
main memory available: 1GB (the main scenario), and 8GB. For each
scenario we have considered instances with 1, 2, 4, 8, 16, and 32
million sequences. The main scenario has four different data sources:
(a) 148bp Illumina reads from the [Genome In A Bottle](https://www.nist.gov/programs-projects/genome-bottle) (GIAB)
consortium, more precisely from the NA24385 individual; (b) random
strings with length 151; (c) Ion Torrent reads from the GIAB
consortium with maximum length 250; (d) simulated sequences with the
GA1 profile.

Since the Illumina dataset have a 300x mean coverage, it is sure
that they contain duplicate reads, hence the value of l (that is, the
maximum value stored in the LCP array) is equal to the length k of the
input strings: this is the worst case for our approach. The random
strings have been generated by a `Python` script that builds uniformly
distributed fixed-length sequences over the DNA alphabet. We will refer
to these datasets as the *random* datasets. The goal of this scenario is
to assess experimentally our theoretical time complexity that shows a
dependency on l. Since the other datasets that we have
analyzed have l equal to the length of the input strings, we needed a
dataset where l is significantly shorter than that. Random strings
couple such property with ease of production. The Ion Torrent dataset is
a real GIAB dataset dataset obtained with a different sequencing
technology. It consists of reads sampled randomly from those [available](ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/ChineseTrio/HG005_NA24631_son/ion_exome)
and obtained from the HG005--NA24631 individual (the son of a Han
Chinese Trio). The reads have a great variance in length: to reduce this
variance we have trimmed all reads to maximum length 250. The maximum
value of the LCP array of this dataset (l) is 250, i.e., this dataset
includes duplicated reads. The GA1 dataset consists of reads that should
resemble those produced by an Illumina machine (GenomeAnalyzer), as
simulated using `art`. This dataset
has been obtained by providing to `art` the chromosome 10 of the human
genome as reference and setting 10 as fold coverage. The lengths of the
reads of this dataset were set by `art` and are equal to 36. The reads
were not corrected and were provided to the tools under analysis as-is.

The second scenario is with 8GB of RAM and its main goal is to show that
in-memory approaches are likely to be unfeasible for larger datasets: in
fact we will show that `gsa-is` is unable to process 8 million strings.
A secondary goal of this scenario is to measure the amount of RAM that
each tool actually uses, as peak memory usage. Since a detailed
comparison in this scenario is not a goal, we have analyzed the tools
only on datasets NA24385 and random.

We ran all the experiments on the same workstation running Ubuntu Linux
18.04 equipped with an Intel Core i7-4770 CPU running at 3.40GHz and a
256GB solid state disk. The machine has 8GB of RAM. In the
first scenario, we have limited the amount of RAM at boot time to 1GB to
avoid the effects of OS caching. To gather the required data, we
analyzed the tools using the system `/usr/bin/time` tool. Although as
long as the main memory usage does not lead to saturation the tool under
analysis is able to obtain its goal, analyzing the memory performance of
the tools is useful since it allows us to better understand their
behavior. Moreover, by keeping track of the peak RAM usage it is
possible to understand whether a tool would fail (or greatly increase
the running time) a specific execution on a different machine equipped
with less memory than the one used for this evaluation.

Main memory usage
-----------------

#### Table 1
                                                                                  
|    l   |  `ble`  |  `BEETL2`          |  `BEETL`          | `gsa-is`  |  `egsa`   |  `egap`   |
 -------:| -------:| ----------------- :| -----------------:| ---------:| ---------:| ---------:
     1M  |       7 |                 35 |               265 |      1312 |     1894  |    730
     2M  |      11 |                 67 |               510 |      2621 |     2099  |   1458
     4M  |      19 |                131 |              1042 |      5240 |     2099  |   2917
     8M  |      34 |                255 |              2166 |   NA      |     2099  |   4197
    16M  |      65 |                483 |              3007 |   NA      |       NA  |   4197
    32M  |     128 |                938 |              3982 |   NA      |       NA  |   4197

* Peak RAM usage to compute the BWT and the LCP (in megabytes). The
  first column indicates the number of sequences in the datasets. Column
  l indicates the maximum value of the LCP array on that dataset.
  NA means that the tool crashed after saturating either the
  available (8GB) RAM or the disk space.*

| abc | defghi |
:-: | -----------:
bar | baz

::: {#table:experimental-evaluation-space}
  ----- ----- ------- ------------------- ------------------ ---------- --------- --------
                                                                                  
        l     `ble`   `BEETL2`   `BEETL`   `gsa-is`    `egsa`   `egap`
     1M             7                  35                247       1338      1932      749
     2M            11                  67                437       2674      2099     1487
     4M            19                 131                859       5346      2099     2971
     8M            34                 255               1780    NA      2099     4197
    16M            65                 483               3379    NA   NA     4197
    32M           128                 938               3982    NA   NA     4197
  ----- ----- ------- ------------------- ------------------ ---------- --------- --------

  :  Peak RAM usage to compute the BWT and the LCP (in megabytes). The
  first column indicates the number of sequences in the datasets. Column
  l indicates the maximum value of the LCP array on that dataset.
  NA means that the tool crashed after saturating either the
  available (8GB) RAM or the disk space.
:::

Table [2](#table:experimental-evaluation-space){reference-type="ref"
reference="table:experimental-evaluation-space"} shows the peak RAM
usage on the NA24385 and the random datasets. The first observation is
the all programs have essentially the same peak RAM usage on both
datasets: for this reason we will focus on the results on the NA24385
dataset.

`ble` is the program that requires the smallest amount of main memory
--- the closest is `BEETL2` which requires from 5 to 7 times
the memory of `ble`, moreover with a widening gap as the instances
become larger. For `egap` we have used the default amount of memory
(4GB). As expected, `gsa-is` requires even more main memory and quickly
saturates the available memory. The last tool analyzed, `egsa`, has a
different behavior. Indeed, it seems to allocate the same amount of main
memory regardless of the size of the input dataset, requiring roughly
2GB for each run. The memory usage of the tools grows proportionally to
the number of input strings (i.e., the input size), except for
`BEETL2` when going from 16M to 32M reads and `egsa` whose
memory usage is essentially capped at 2GB, as stated previously. Notice
that `ble`, BEETL, BEETL2, and `egap` are the only
tools able to successfully process 32M reads.

From
Table [2](#table:experimental-evaluation-space){reference-type="ref"
reference="table:experimental-evaluation-space"} we infer that
`BEETL` requires 4GB of RAM to process the 32M dataset, which
can be processed with 1GB by `BEETL2` and with 0.1GB by `ble`.
While `egap` shows a peak memory usage of 4GB, it can be run with 1GB of
RAM, as we will show in the next section.

Results with 1GB of available RAM 
---------------------------------

The results of the runs with 1GB of available RAM are summarized in
Table [4](#table:experimental-evaluation-times-1g){reference-type="ref"
reference="table:experimental-evaluation-times-1g"}. We expect those
results to show the advantages of memory-conscious approaches. In this
scenario, we have allowed `egap` to use 95% of the available RAM, as
suggested in its website.

::: {#table:experimental-evaluation-times-1g}
  ----- ----- ------- ------------------- ------------------ ---------- --------- --------
                                                                                  
        l     `ble`   `BEETL2`   `BEETL`   `gsa-is`    `egsa`   `egap`
     1M            25                  12                 10         55   NA        6
     2M            54                  27                 25    NA   NA       30
     4M           105                  55                 50    NA   NA       90
     8M           220                 106                118    NA   NA      305
    16M           470                 260                272    NA   NA      618
    32M           831                1335            NA    NA   NA     1086
  ----- ----- ------- ------------------- ------------------ ---------- --------- --------

  : Time required to compute the BWT and the LCP array (in minutes) on
  the NA24385 and random datasets using a PC with 1GB RAM. The first
  column indicates the number of sequences in the datasets. Column l
  indicates the maximum value of the LCP array on that dataset. NA
  means that the tool either crashed or was killed after 50 hours.
  `gsa-is` and `egsa` could not compute the results on most of the
  datasets. `BEETL` and `BEETL2` were able to compute
  most of the results with good performance but either crashed or
  greatly degraded their performance on the bigger datasets. `ble` and
  `egap` were the only tools able to compute the BWT and the LCP array
  of all the datasets. On all instances of the random dataset, and on
  the 32M instance of the NA24385 dataset, `ble` was the fastest tool.
:::

::: {#table:experimental-evaluation-times-1g}
  ----- ----- ------- ------------------- ------------------ ---------- --------- --------
                                                                                  
        l     `ble`   `BEETL2`   `BEETL`   `gsa-is`    `egsa`   `egap`
     1M             5                  13                 12         76      2400        8
     2M            11                  28                 26    NA   NA       24
     4M            21                  57                 54    NA   NA       72
     8M            46                 114                110    NA   NA      190
    16M            98                 220                230    NA   NA      396
    32M           208             NA            NA    NA   NA      950
  ----- ----- ------- ------------------- ------------------ ---------- --------- --------

  : Time required to compute the BWT and the LCP array (in minutes) on
  the NA24385 and random datasets using a PC with 1GB RAM. The first
  column indicates the number of sequences in the datasets. Column l
  indicates the maximum value of the LCP array on that dataset. NA
  means that the tool either crashed or was killed after 50 hours.
  `gsa-is` and `egsa` could not compute the results on most of the
  datasets. `BEETL` and `BEETL2` were able to compute
  most of the results with good performance but either crashed or
  greatly degraded their performance on the bigger datasets. `ble` and
  `egap` were the only tools able to compute the BWT and the LCP array
  of all the datasets. On all instances of the random dataset, and on
  the 32M instance of the NA24385 dataset, `ble` was the fastest tool.
:::

::: {#table:experimental-evaluation-times-1g-ion-torrent}
  ----- ----- ------- ------------------- ------------------ ---------- --------- ---------
                                                                                  
        l     `ble`   `BEETL2`   `BEETL`   `gsa-is`    `egsa`    `egap`
     1M            58                  27            NA    NA        68       292
     2M           118                  72            NA    NA   NA       537
     4M           238                 206            NA    NA   NA      1169
     8M           478                 661            NA    NA   NA      1431
    16M           927             NA            NA    NA   NA      2798
    32M          1868             NA            NA    NA   NA   NA
  ----- ----- ------- ------------------- ------------------ ---------- --------- ---------

  : Time required to compute the BWT and the LCP array (in minutes) on
  the Ion Torrent dataset using a PC with 1GB RAM. The first column
  indicates the number of sequences in the datasets. Column l
  indicates the maximum value of the LCP array on that dataset. NA
  means that the tool either crashed or was killed after 50 hours. `ble`
  was the only tool able to compute the BWT and the LCP array of all the
  datasets and was the fastest on instances with at least 8 million
  reads. `BEETL` cannot analyze a dataset including sequences
  with different lengths and was included in this table for consistency
  with the previous ones.
:::

::: {#table:experimental-evaluation-times-1g-GA1}
  ----- ----- ------- ------------------- ------------------ ---------- --------- ---------
                                                                                  
        l     `ble`   `BEETL2`   `BEETL`   `gsa-is`    `egsa`    `egap`
     1M             2                   1            NA    NA    0 (9s)    0 (9s)
     2M             4                   4            NA    NA   0 (20s)   0 (20s)
     4M             6                   4            NA    NA        23        13
     8M            14                  12            NA    NA       850       599
    16M            27                  20            NA    NA   NA      1761
    32M            52             NA            NA    NA   NA   NA
  ----- ----- ------- ------------------- ------------------ ---------- --------- ---------

  : Time required to compute the BWT and the LCP array (in minutes) on
  the GA1 simulated dataset using a PC with 1GB RAM. The first column
  indicates the number of sequences in the datasets. Column l
  indicates the maximum value of the LCP array for each dataset. NA
  means that the tool either crashed or was killed after 50 hours. `ble`
  was the only tool able to compute the BWT and the LCP array of all the
  datasets, since `BEETL2` and `egap` have failed on 32 million
  reads. While `BEETL2` was the fastest tool on smaller
  instances, `ble` had comparable performances.
:::

The results confirm that only `ble`, BEETL,
BEETL2, and `egap` are able to deal with such a limited
amount of main memory. Moreover, only `ble` was able to compute the BWT
and LCP array for all datasets, with `egap` only failing on 32M Ion
Torrent, and GA1 datasets (that is, with 32 million reads), and all
other tools failing on more datasets. More precisely, `gsa-is` could
only complete the analysis of the NA24385 and random datasets composed
of 1M reads and was killed after 50 hours in all the remaining runs.
`egsa` required roughly 40 hours to analyze the 1M random dataset,
68 minutes to analyze the 1M Ion Torrent datasets, and less than
25 minutes to analyze the GA1 dataset of up to 4M reads. `egsa`
could not efficiently manage datasets with more reads; indeed it
required more than 13 hours to build the BWT and the LCP array of the
GA1 dataset composed of 8M reads and failed to manage all the
remaining datasets.

On the NA24385 dataset, `BEETL` and `BEETL2` show
similar performance and are twice as fast as `ble`, apart from the 1M
and 32M datasets, where `ble` is roughly 1.5 times faster than
BEETL2(note that `BEETL` crashed on the 32M
dataset). On this instance, `egap` is the fastest tool.

Notice that, except for 1M reads, the runtime of `ble` is proportional
to the number of reads. The runtime of `BEETL2` exhibits a
similar behavior, except for 32M reads, whose runtime is approximately
4 times that on 16M reads. Again, together with the peak RAM usage
(Table [2](#table:experimental-evaluation-space){reference-type="ref"
reference="table:experimental-evaluation-space"}), this fact suggests
that `BEETL2` saturates the available RAM and starts swapping.
This conjecture is also consistent with the fact that BEETL2
is unable to process 32M reads on the other datasets.

The results on the random dataset confirm that `ble` has a time
complexity that depends on the maximum value of the LCP array, while the
other tools depend on the maximum length of the input strings. On this
dataset, `ble` is at least twice as fast as the other tools --- except
for `egap` on 1 million strings.

The Ion Torrent datasets highlight that `ble` is the only tool that is
able to cope with datasets larger than the available RAM. Indeed,
`BEETL2` was able to complete its analysis only on the datasets
with at most 8M reads, `egsa` failed to manage more than 1M reads,
`gsa-is` crashed on all the instances, and `egap` is the only other tool
able to succeed on 16 million strings. The results of the tools on the
Ion Torrent datasets are shown in
Table [5](#table:experimental-evaluation-times-1g-ion-torrent){reference-type="ref"
reference="table:experimental-evaluation-times-1g-ion-torrent"}.

The GA1 datasets shows consistent results with those on the Ion Torrent
datasets. Indeed, only `ble`, BEETL2, and `egap` have been
able to deal with larger datasets, whereas `egsa` could only manage
datasets of up to 4M reads, and `gsa-is` and `BEETL` could
not complete any analysis. Conversely to the Ion Torrent datasets,
`egsa` completed the analysis of 3 more datasets, although the
performance degraded considerably on the 8M dataset. This is due to
the total amount of bases included in the datasets; indeed, the lengths
of the reads in the GA1 datasets are considerably smaller than the ones
of the Ion Torrent datasets. Still, `ble` is the only tool that has
succeeded on 32 million strings.

Overall, the results presented in this section show that `ble` is
currently the only tool that is able to manage datasets that are
sufficiently larger than the available amount of main memory. Even in
its current prototype form, our tool shows reasonable performance and
can be used to efficiently index a set of strings using a limited amount
of resources (i.e., even on microservers), where the other tools are
unable to succeed.

Moreover, the fact that RAM usage is essentially proportional to the
number of input strings, allows us to infer a projection for a larger
amount of available RAM. We expect that, on the NA24385 dataset,
`BEETL2` needs 28Kbytes for each input string, while `ble`
needs 4Kbytes for each input reads.

The analysis of the running times on the NA24385 datasets shows that our
tool requires between twice and three times the runtimes of
`BEETL` and BEETL2, that in turn require between
twice and three times the runtimes of `egsa`. As expected, `ble`
performance improves when l is smaller than the length of the
sequences in the dataset. In fact, `ble` requires roughly half of the
runtimes of `BEETL` and `BEETL2` and between 1 and
1.5 the runtimes of `egsa` on the 1M, 2M, 4M, and 8M random
datasets, whereas the runtimes are similar to the ones of
`BEETL` and `BEETL2` on bigger datasets. `egap` is
slightly slower than `BEETL` and BEETL2, except on 1
million reads, where it outperforms both (but not `ble`). This is likely
due to a different management of the cache and disk buffers between the
programs. The in-memory `gsa-is` is significantly faster than `ble`,
BEETL, BEETL2, and `egsa`, requiring up to 20
times less time to compute the two data structures, but crashed on the
8M, 16M, and 32M instances of both datasets.

Results with 8GB of available RAM
---------------------------------

The results of the runs with 8GB of main memory available are summarized
in Table [8](#table:experimental-evaluation-times){reference-type="ref"
reference="table:experimental-evaluation-times"}. The goal of this
section is to prove that in-memory tools require a very large amount of
RAM on most real-world datasets, which can easily contain 500 million
reads. In fact, `gsa-is` has been unable to process 8 million reads.

Table [2](#table:experimental-evaluation-space){reference-type="ref"
reference="table:experimental-evaluation-space"} hints that none of the
external-memory tools actually uses 8GB of RAM. Anyway, the operating
system likely uses the available RAM as disk cache, therefore the
results of this comparison cannot be as clear as the ones with 1GB of
RAM. Moreover, we expect that tools using more RAM are also faster,
which is essentially confirmed by this experiment. In fact, the fastest
tool on each instance is either `egap` or `BEETL2` and they are
also almost always the two tools using more memory.

In a few instances (5 for `ble` and 1 for `BEETL2`) the time
needed with 8GB of RAM is more than that with 1GB, which is unexpected.
In all those cases, `BEETL2` and `ble` used much less than 1GB
of RAM which makes the extra amount of RAM essentially irrelevant.
Observe that compared to `BEETL2` our tool `ble` uses
significantly less main memory. Since the runtime is subject to natural
variations, we expect the runtime to be approximately the same in both
settings --- which is what we observed.

::: {#table:experimental-evaluation-times}
  ----- ----- ------- ------------------- ------------------ ---------- --------- --------
                                                                                  
        l     `ble`   `BEETL2`   `BEETL`   `gsa-is`    `egsa`   `egap`
     1M            16                   7                  9          1         3        1
     2M            33                  15                 20          2         6        2
     4M            91                  30                 46          5        14        3
     8M           245                  97                105    NA        31       50
    16M           420                 194                225    NA   NA      108
    32M           841                 426                367    NA   NA      604
  ----- ----- ------- ------------------- ------------------ ---------- --------- --------

  : Time required to compute the BWT and the LCP array (in minutes) on
  the NA24385 and random datasets using a PC with 8GB RAM. The first
  column indicates the number of sequences in the datasets. Column l
  indicates the maximum value of the LCP array on that dataset. NA
  means that the tool crashed after saturating either the available RAM
  or the disk space. `gsa-is` is by far the fastest tool but is unable
  to manage datasets with more than 4M sequences.
:::

::: {#table:experimental-evaluation-times}
  ----- ----- ------- ------------------- ------------------ ---------- --------- --------
                                                                                  
        l     `ble`   `BEETL2`   `BEETL`   `gsa-is`    `egsa`   `egap`
     1M             3                   8                  9          1         3        1
     2M             7                  16                 20          3         5        2
     4M            18                  32                 50          6        11        4
     8M            51                 109                107    NA        24       15
    16M           104                 223                189    NA       *       31
    32M           212                 395                532    NA   NA      120
  ----- ----- ------- ------------------- ------------------ ---------- --------- --------

  : Time required to compute the BWT and the LCP array (in minutes) on
  the NA24385 and random datasets using a PC with 8GB RAM. The first
  column indicates the number of sequences in the datasets. Column l
  indicates the maximum value of the LCP array on that dataset. NA
  means that the tool crashed after saturating either the available RAM
  or the disk space. `gsa-is` is by far the fastest tool but is unable
  to manage datasets with more than 4M sequences.
:::

[^1]: The second version is available at
    <>.
